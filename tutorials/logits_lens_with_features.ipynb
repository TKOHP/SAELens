{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6UyzFGfBSdA"
   },
   "source": [
    "# Understanding SAE Features with the Logit Lens\n",
    "\n",
    "This notebook demonstrates how to use the mats_sae_training library to perform the analysis documented the post \"[Understanding SAE Features with the Logit Lens](https://www.alignmentforum.org/posts/qykrYY6rXXM7EEs8Q/understanding-sae-features-with-the-logit-lens)\".\n",
    "\n",
    "As such, the notebook will include sections for:\n",
    "- Loading in GPT2-Small Residual Stream SAEs from Huggingface.\n",
    "- Performing Virtual Weight Based Analysis of features (specifically looking at the logit weight distributions).\n",
    "- Programmatically opening neuronpedia tabs to engage with public dashboards on [neuronpedia](https://www.neuronpedia.org/).\n",
    "- Performing Token Set Enrichment Analysis (based on Gene Set Enrichment Analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff-PciWXBSdB"
   },
   "source": [
    "## Set Up\n",
    "\n",
    "Here we'll load various functions for things like:\n",
    "- downloading and loading our SAEs from huggingface.\n",
    "- opening neuronpedia from a jupyter cell.\n",
    "- calculating statistics of the logit weight distributions.\n",
    "- performing Token Set Enrichment Analysis (TSEA) and plotting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vImmTg-8BSdC"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T01:13:43.998818600Z",
     "start_time": "2024-07-29T01:13:43.987818500Z"
    },
    "id": "sOd2C0e1BfN1"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # For Google Colab, a high RAM instance is needed\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    %pip install sae-lens transformer-lens\n",
    "except:\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T01:23:23.332321900Z",
     "start_time": "2024-07-29T01:23:23.299315Z"
    },
    "id": "QmdAd_25BSdC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Got an error when attempting to re.compile('C:\\Users\\11349\\AppData\\Roaming\\config2py\\configs\\'): <class 're.error'>(incomplete escape \\U at position 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\dol\\naming.py:327\u001b[0m, in \u001b[0;36mmk_pattern_from_template_and_format_dict\u001b[1;34m(template, format_dict, sep)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\re.py:251\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompile a regular expression pattern, returning a Pattern object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\re.py:303\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst argument must be string or compiled pattern\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 303\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msre_compile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags \u001b[38;5;241m&\u001b[39m DEBUG):\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\sre_compile.py:788\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    787\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m--> 788\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43msre_parse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\sre_parse.py:955\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 955\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_sub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSRE_FLAG_VERBOSE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Verbose:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;66;03m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;66;03m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\sre_parse.py:444\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     itemsappend(\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sourcematch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\sre_parse.py:526\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m this[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 526\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[43m_escape\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m     subpatternappend(code)\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\sre_parse.py:382\u001b[0m, in \u001b[0;36m_escape\u001b[1;34m(source, escape, state)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(escape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincomplete escape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m escape, \u001b[38;5;28mlen\u001b[39m(escape))\n\u001b[0;32m    383\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(escape[\u001b[38;5;241m2\u001b[39m:], \u001b[38;5;241m16\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: incomplete escape \\U at position 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msae_lens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_statistics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     get_all_stats_dfs,\n\u001b[0;32m     14\u001b[0m     get_W_U_W_dec_stats_df,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Enrichment Analysis Functions\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msae_lens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsea\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     get_enrichment_df,\n\u001b[0;32m     20\u001b[0m     manhattan_plot_enrichment_scores,\n\u001b[0;32m     21\u001b[0m     plot_top_k_feature_projections_by_token_and_category,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msae_lens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsea\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     get_baby_name_sets,\n\u001b[0;32m     25\u001b[0m     get_letter_gene_sets,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     get_gene_set_from_regex,\n\u001b[0;32m     29\u001b[0m )\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\sae_lens\\analysis\\tsea.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly_express\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbabe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UsNames\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedTransformer\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_enrichment_df\u001b[39m(\n\u001b[0;32m     16\u001b[0m     projections: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m     17\u001b[0m     features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m     18\u001b[0m     gene_sets_selected: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mint\u001b[39m]],\n\u001b[0;32m     19\u001b[0m ):\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\babe\\__init__.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraze\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Graze\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy2store\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FilesOfZip, filt_iter, wrap_kvs\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# us_names_src_url = \"http://www.ssa.gov/oact/babynames/state/namesbystate.zip\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#     \"\"\"Get gender of name\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#     return __gender_of_name().get(name, None)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mUsNames\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\py2store\\__init__.py:41\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy2store\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     Collection,\n\u001b[0;32m     32\u001b[0m     KvReader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     Store,\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy2store\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpersisters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocal_files\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileReader\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy2store\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrabbers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ipython_display_val_trans\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy2store\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstores\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocal_store\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     44\u001b[0m     LocalStore,\n\u001b[0;32m     45\u001b[0m     LocalBinaryStore,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     PickleStore,  \u001b[38;5;66;03m# consider deprecating and use LocalPickleStore instead?\u001b[39;00m\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy2store\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstores\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocal_store\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     52\u001b[0m     QuickStore,\n\u001b[0;32m     53\u001b[0m     QuickBinaryStore,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     QuickPickleStore,\n\u001b[0;32m     57\u001b[0m )\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\py2store\\my\\grabbers.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wraps\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BytesIO, StringIO\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy2store\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_obj\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmk_grabber\u001b[39m(\u001b[38;5;241m*\u001b[39m, key_trans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, val_trans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;129m@wraps\u001b[39m(get_obj)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab\u001b[39m(k, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\py2store\\misc.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mFunctions to read from and write to misc sources.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mForwards to dol.misc\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdol\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\dol\\misc.py:66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# TODO: Change whole module to be proper plugin architecture\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig2py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigReader, ConfigStore\n\u001b[0;32m     68\u001b[0m     dflt_incoming_val_trans_for_key[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.ini\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m partial(\n\u001b[0;32m     69\u001b[0m         ConfigStore, interpolation\u001b[38;5;241m=\u001b[39mConfigReader\u001b[38;5;241m.\u001b[39mExtendedInterpolation(),\n\u001b[0;32m     70\u001b[0m     )\n\u001b[0;32m     72\u001b[0m     dflt_outgoing_val_trans_for_key[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.ini\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m partial(\n\u001b[0;32m     73\u001b[0m         ConfigStore, interpolation\u001b[38;5;241m=\u001b[39mConfigReader\u001b[38;5;241m.\u001b[39mExtendedInterpolation()\n\u001b[0;32m     74\u001b[0m     )\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\config2py\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Tools to read and write configurations from various sources and formats\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig2py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ms_configparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfigStore, ConfigReader\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig2py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     extract_exports,\n\u001b[0;32m      6\u001b[0m     get_configs_local_store,\n\u001b[0;32m      7\u001b[0m     simple_config_getter,\n\u001b[0;32m      8\u001b[0m     config_getter,\n\u001b[0;32m      9\u001b[0m     local_configs,\n\u001b[0;32m     10\u001b[0m     Configs,  \u001b[38;5;66;03m# user-customized configs store class (default is TextFiles)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     configs,  \u001b[38;5;66;03m# user-customized configs store instance (default is local_configs)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# or make configs be more of a default get_configs \"chained Mapping\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig2py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config, user_gettable, sources_chainmap\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig2py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     ask_user_for_input,\n\u001b[0;32m     17\u001b[0m     get_app_data_folder,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     process_path,\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\config2py\\tools.py:106\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_getter\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Make a ready-to-use config getter, using the defaults\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m config_getter \u001b[38;5;241m=\u001b[39m \u001b[43msimple_config_getter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Ready to import instances\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m#   Note: Perhaps there's no need for a class.\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m#   Maybe just a function that returns a store\u001b[39;00m\n\u001b[0;32m    118\u001b[0m Configs \u001b[38;5;241m=\u001b[39m TextFiles  \u001b[38;5;66;03m# TODO: deprecate\u001b[39;00m\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\config2py\\tools.py:89\u001b[0m, in \u001b[0;36msimple_config_getter\u001b[1;34m(configs_src, first_look_in_env_vars, ask_user_if_key_not_found, config_store_factory)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make a simple config getter from a \"central\" config source specification.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03mThe purpose of this function is to implement a common pattern of getting configs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m    and returns the central config store\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# TODO: Resource validation block. Refactor! And add tool to config2py if not there\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m central_configs \u001b[38;5;241m=\u001b[39m \u001b[43mconfig_store_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs_src\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m sources \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_look_in_env_vars:\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\config2py\\tools.py:42\u001b[0m, in \u001b[0;36mget_configs_local_store\u001b[1;34m(config_src, configs_name)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the local store of configs.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m:param config_src: A specification of the local config store. By default:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    If it's a string, it's assumed to be an app name, from which to create a folder\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep \u001b[38;5;129;01min\u001b[39;00m config_src \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(config_src):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# TODO: This was a quick fix to avoid unknowingly making directories in the\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m#   wrong place. Broke stuff so leaving this for later.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m#         \"contain a {config_src} folder.\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTextFiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_src\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(config_src):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# TODO: Not tested\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# TODO: Make this open-closed plug-in via routing argument\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     _, extension \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(config_src)\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\dol\\paths.py:822\u001b[0m, in \u001b[0;36mmk_relative_path_store.<locals>.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(store_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m)\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 822\u001b[0m     Store\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, store\u001b[38;5;241m=\u001b[39mstore_cls(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    823\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m recursive_get_attr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore, prefix_attr, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\n\u001b[0;32m    825\u001b[0m         \u001b[38;5;28mself\u001b[39m, prefix_attr, prefix\n\u001b[0;32m    826\u001b[0m     )\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\dol\\filesys.py:216\u001b[0m, in \u001b[0;36mFileSysCollection.__init__\u001b[1;34m(self, rootdir, subpath, pattern_for_field, max_levels, include_hidden, assert_rootdir_existence)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrootdir \u001b[38;5;241m=\u001b[39m ensure_slash_suffix(rootdir)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubpath \u001b[38;5;241m=\u001b[39m subpath\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_pattern \u001b[38;5;241m=\u001b[39m \u001b[43mmk_pattern_from_template_and_format_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrootdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern_for_field\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_levels \u001b[38;5;241m=\u001b[39m max_levels\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_hidden \u001b[38;5;241m=\u001b[39m include_hidden\n",
      "File \u001b[1;32mD:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\dol\\naming.py:329\u001b[0m, in \u001b[0;36mmk_pattern_from_template_and_format_dict\u001b[1;34m(template, format_dict, sep)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39mcompile(pattern)\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot an error when attempting to re.compile(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    332\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Got an error when attempting to re.compile('C:\\Users\\11349\\AppData\\Roaming\\config2py\\configs\\'): <class 're.error'>(incomplete escape \\U at position 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import plotly_express as px\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# Model Loading\n",
    "from sae_lens import SAE\n",
    "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "\n",
    "# Virtual Weight / Feature Statistics Functions\n",
    "from sae_lens.analysis.feature_statistics import (\n",
    "    get_all_stats_dfs,\n",
    "    get_W_U_W_dec_stats_df,\n",
    ")\n",
    "\n",
    "# Enrichment Analysis Functions\n",
    "from sae_lens.analysis.tsea import (\n",
    "    get_enrichment_df,\n",
    "    manhattan_plot_enrichment_scores,\n",
    "    plot_top_k_feature_projections_by_token_and_category,\n",
    ")\n",
    "from sae_lens.analysis.tsea import (\n",
    "    get_baby_name_sets,\n",
    "    get_letter_gene_sets,\n",
    "    generate_pos_sets,\n",
    "    get_test_gene_sets,\n",
    "    get_gene_set_from_regex,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VEAe5FjBSdD"
   },
   "source": [
    "### Loading GPT2 Small and SAE Weights\n",
    "\n",
    "This will take a while the first time you run it, but will be quick thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T01:23:55.553199800Z",
     "start_time": "2024-07-29T01:23:44.945707Z"
    },
    "id": "HQ904zDOBSdD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# # Add domestic mirror path\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "# this is an outdated way to load the SAE. We need to have feature spartisity loadable through the new interface to remove it.\n",
    "gpt2_small_sparse_autoencoders = {}\n",
    "gpt2_small_sae_sparsities = {}\n",
    "\n",
    "for layer in range(12):\n",
    "    sae, original_cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gpt2-small-res-jb\",\n",
    "        sae_id=\"blocks.0.hook_resid_pre\",\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "    gpt2_small_sparse_autoencoders[f\"blocks.{layer}.hook_resid_pre\"] = sae\n",
    "    gpt2_small_sae_sparsities[f\"blocks.{layer}.hook_resid_pre\"] = sparsity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pFYJKeNBSdD"
   },
   "source": [
    "# Statistical Properties of Feature Logit Distributions\n",
    "\n",
    "In the post I study layer 8 (for no particular reason). At the end of this notebook is code for visualizing these statistics across all layers. Feel free to change the layer here and explore different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEK1jEpEBSdD"
   },
   "outputs": [],
   "source": [
    "# In the post, I focus on layer 8\n",
    "layer = 8\n",
    "\n",
    "# get the corresponding SAE and feature sparsities.\n",
    "sparse_autoencoder = gpt2_small_sparse_autoencoders[f\"blocks.{layer}.hook_resid_pre\"]\n",
    "log_feature_sparsity = gpt2_small_sae_sparsities[f\"blocks.{layer}.hook_resid_pre\"].cpu()\n",
    "\n",
    "W_dec = sparse_autoencoder.W_dec.detach().cpu()\n",
    "\n",
    "# calculate the statistics of the logit weight distributions\n",
    "W_U_stats_df_dec, dec_projection_onto_W_U = get_W_U_W_dec_stats_df(\n",
    "    W_dec, model, cosine_sim=False\n",
    ")\n",
    "W_U_stats_df_dec[\"sparsity\"] = (\n",
    "    log_feature_sparsity  # add feature sparsity since it is often interesting.\n",
    ")\n",
    "display(W_U_stats_df_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GK56PDl3BSdE"
   },
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the 3rd / 4th moments. I found these aren't as useful on their own as joint distributions can be.\n",
    "px.histogram(\n",
    "    W_U_stats_df_dec,\n",
    "    x=\"skewness\",\n",
    "    width=800,\n",
    "    height=300,\n",
    "    nbins=1000,\n",
    "    title=\"Skewness of the Logit Weight Distributions\",\n",
    ").show()\n",
    "\n",
    "px.histogram(\n",
    "    W_U_stats_df_dec,\n",
    "    x=np.log10(W_U_stats_df_dec[\"kurtosis\"]),\n",
    "    width=800,\n",
    "    height=300,\n",
    "    nbins=1000,\n",
    "    title=\"Kurtosis of the Logit Weight Distributions\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_6c12ftBSdE"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    W_U_stats_df_dec,\n",
    "    x=\"skewness\",\n",
    "    y=\"kurtosis\",\n",
    "    color=\"std\",\n",
    "    color_continuous_scale=\"Portland\",\n",
    "    hover_name=\"feature\",\n",
    "    width=800,\n",
    "    height=500,\n",
    "    log_y=True,  # Kurtosis has larger outliers so logging creates a nicer scale.\n",
    "    labels={\"x\": \"Skewness\", \"y\": \"Kurtosis\", \"color\": \"Standard Deviation\"},\n",
    "    title=f\"Layer {8}: Skewness vs Kurtosis of the Logit Weight Distributions\",\n",
    ")\n",
    "\n",
    "# decrease point size\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuWSPqqjBSdE"
   },
   "outputs": [],
   "source": [
    "# then you can query accross combinations of the statistics to find features of interest and open them in neuronpedia.\n",
    "tmp_df = W_U_stats_df_dec[[\"feature\", \"skewness\", \"kurtosis\", \"std\"]]\n",
    "# tmp_df = tmp_df[(tmp_df[\"std\"] > 0.04)]\n",
    "# tmp_df = tmp_df[(tmp_df[\"skewness\"] > 0.65)]\n",
    "tmp_df = tmp_df[(tmp_df[\"skewness\"] > 3)]\n",
    "tmp_df = tmp_df.sort_values(\"skewness\", ascending=False).head(10)\n",
    "display(tmp_df)\n",
    "\n",
    "# if desired, open the features in neuronpedia\n",
    "get_neuronpedia_quick_list(list(tmp_df.feature), layer=layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHGG86wkBSdE"
   },
   "source": [
    "# Token Set Enrichment Analysis\n",
    "\n",
    "We now proceed to token set enrichment analysis.  I highly recommend reading my AlignmentForum post (espeically the case studies) before reading too much into any of these results.\n",
    "Also read this [post](https://transformer-circuits.pub/2024/qualitative-essay/index.html) for good general perspectives on statistics here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4tuqT9SBSdE"
   },
   "source": [
    "## Defining Our Token Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mUxqd6mBSdE"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "# get the vocab we need to filter to formulate token sets.\n",
    "vocab = model.tokenizer.get_vocab()  # type: ignore\n",
    "\n",
    "# make a regex dictionary to specify more sets.\n",
    "regex_dict = {\n",
    "    \"starts_with_space\": r\"Ġ.*\",\n",
    "    \"starts_with_capital\": r\"^Ġ*[A-Z].*\",\n",
    "    \"starts_with_lower\": r\"^Ġ*[a-z].*\",\n",
    "    \"all_digits\": r\"^Ġ*\\d+$\",\n",
    "    \"is_punctuation\": r\"^[^\\w\\s]+$\",\n",
    "    \"contains_close_bracket\": r\".*\\).*\",\n",
    "    \"contains_open_bracket\": r\".*\\(.*\",\n",
    "    \"all_caps\": r\"Ġ*[A-Z]+$\",\n",
    "    \"1 digit\": r\"Ġ*\\d{1}$\",\n",
    "    \"2 digits\": r\"Ġ*\\d{2}$\",\n",
    "    \"3 digits\": r\"Ġ*\\d{3}$\",\n",
    "    \"4 digits\": r\"Ġ*\\d{4}$\",\n",
    "    \"length_1\": r\"^Ġ*\\w{1}$\",\n",
    "    \"length_2\": r\"^Ġ*\\w{2}$\",\n",
    "    \"length_3\": r\"^Ġ*\\w{3}$\",\n",
    "    \"length_4\": r\"^Ġ*\\w{4}$\",\n",
    "    \"length_5\": r\"^Ġ*\\w{5}$\",\n",
    "}\n",
    "\n",
    "# print size of gene sets\n",
    "all_token_sets = get_letter_gene_sets(vocab)\n",
    "for key, value in regex_dict.items():\n",
    "    gene_set = get_gene_set_from_regex(vocab, value)\n",
    "    all_token_sets[key] = gene_set\n",
    "\n",
    "# some other sets that can be interesting\n",
    "baby_name_sets = get_baby_name_sets(vocab)\n",
    "pos_sets = generate_pos_sets(vocab)\n",
    "arbitrary_sets = get_test_gene_sets(model)\n",
    "\n",
    "all_token_sets = {**all_token_sets, **pos_sets}\n",
    "all_token_sets = {**all_token_sets, **arbitrary_sets}\n",
    "all_token_sets = {**all_token_sets, **baby_name_sets}\n",
    "\n",
    "# for each gene set, convert to string and  print the first 5 tokens\n",
    "for token_set_name, gene_set in sorted(\n",
    "    all_token_sets.items(), key=lambda x: len(x[1]), reverse=True\n",
    "):\n",
    "    tokens = [model.to_string(id) for id in list(gene_set)][:10]  # type: ignore\n",
    "    print(f\"{token_set_name}, has {len(gene_set)} genes\")\n",
    "    print(tokens)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxctX05KBSdE"
   },
   "source": [
    "## Performing Token Set Enrichment Analysis\n",
    "\n",
    "Below we perform token set enrichment analysis on various token sets. In practice, we'd likely perform tests accross all tokens and large libraries of sets simultaneously but to make it easier to run, we look at features with higher skew and select of a few token sets at a time to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHwM7qVlBSdF"
   },
   "outputs": [],
   "source": [
    "features_ordered_by_skew = (\n",
    "    W_U_stats_df_dec[\"skewness\"].sort_values(ascending=False).head(5000).index.to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxuiBx4NBSdF"
   },
   "outputs": [],
   "source": [
    "# filter our list.\n",
    "token_sets_index = [\n",
    "    \"starts_with_space\",\n",
    "    \"starts_with_capital\",\n",
    "    \"all_digits\",\n",
    "    \"is_punctuation\",\n",
    "    \"all_caps\",\n",
    "]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "\n",
    "# calculate the enrichment scores\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U,  # use the logit weight values as our rankings over tokens.\n",
    "    features_ordered_by_skew,  # subset by these features\n",
    "    token_set_selected,  # use token_sets\n",
    ")\n",
    "\n",
    "manhattan_plot_enrichment_scores(\n",
    "    df_enrichment_scores, label_threshold=0, top_n=3  # use our enrichment scores\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQ0n0aKTBSdF"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df_enrichment_scores.apply(lambda x: -1 * np.log(1 - x)).T,\n",
    "    x=\"starts_with_space\",\n",
    "    y=\"starts_with_capital\",\n",
    "    marginal_x=\"histogram\",\n",
    "    marginal_y=\"histogram\",\n",
    "    labels={\n",
    "        \"starts_with_space\": \"Starts with Space\",\n",
    "        \"starts_with_capital\": \"Starts with Capital\",\n",
    "    },\n",
    "    title=\"Enrichment Scores for Starts with Space vs Starts with Capital\",\n",
    "    height=800,\n",
    "    width=800,\n",
    ")\n",
    "# reduce point size on the scatter only\n",
    "fig.update_traces(marker=dict(size=2), selector=dict(mode=\"markers\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcmU_6I9BSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"1 digit\", \"2 digits\", \"3 digits\", \"4 digits\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BboYni5ZBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"nltk_pos_PRP\", \"nltk_pos_VBZ\", \"nltk_pos_NNP\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKrcnE7GBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"nltk_pos_VBN\", \"nltk_pos_VBG\", \"nltk_pos_VB\", \"nltk_pos_VBD\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKVdyyxoBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"nltk_pos_WP\", \"nltk_pos_RBR\", \"nltk_pos_WDT\", \"nltk_pos_RB\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYC3GFscBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"a\", \"e\", \"i\", \"o\", \"u\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvmgQ_YmBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"negative_words\", \"positive_words\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltYmy5lMBSdF"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df_enrichment_scores.apply(lambda x: -1 * np.log(1 - x))\n",
    "    .T.reset_index()\n",
    "    .rename(columns={\"index\": \"feature\"}),\n",
    "    x=\"negative_words\",\n",
    "    y=\"positive_words\",\n",
    "    marginal_x=\"histogram\",\n",
    "    marginal_y=\"histogram\",\n",
    "    labels={\n",
    "        \"starts_with_space\": \"Starts with Space\",\n",
    "        \"starts_with_capital\": \"Starts with Capital\",\n",
    "    },\n",
    "    title=\"Enrichment Scores for Starts with Space vs Starts with Capital\",\n",
    "    height=800,\n",
    "    width=800,\n",
    "    hover_name=\"feature\",\n",
    ")\n",
    "# reduce point size on the scatter only\n",
    "fig.update_traces(marker=dict(size=2), selector=dict(mode=\"markers\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5otoyu2SBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"contains_close_bracket\", \"contains_open_bracket\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlN-ScWhBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\n",
    "    \"1910's\",\n",
    "    \"1920's\",\n",
    "    \"1930's\",\n",
    "    \"1940's\",\n",
    "    \"1950's\",\n",
    "    \"1960's\",\n",
    "    \"1970's\",\n",
    "    \"1980's\",\n",
    "    \"1990's\",\n",
    "    \"2000's\",\n",
    "    \"2010's\",\n",
    "]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDmCs9OhBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"positive_words\", \"negative_words\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores, label_threshold=0.98).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilzC33VlBSdF"
   },
   "outputs": [],
   "source": [
    "token_sets_index = [\"boys_names\", \"girls_names\"]\n",
    "token_set_selected = {\n",
    "    k: set(v) for k, v in all_token_sets.items() if k in token_sets_index\n",
    "}\n",
    "df_enrichment_scores = get_enrichment_df(\n",
    "    dec_projection_onto_W_U, features_ordered_by_skew, token_set_selected\n",
    ")\n",
    "manhattan_plot_enrichment_scores(df_enrichment_scores).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0mTCic7BSdG"
   },
   "outputs": [],
   "source": [
    "tmp_df = df_enrichment_scores.apply(lambda x: -1 * np.log(1 - x)).T\n",
    "color = (\n",
    "    W_U_stats_df_dec.sort_values(\"skewness\", ascending=False)\n",
    "    .head(5000)[\"skewness\"]\n",
    "    .values\n",
    ")\n",
    "fig = px.scatter(\n",
    "    tmp_df.reset_index().rename(columns={\"index\": \"feature\"}),\n",
    "    x=\"boys_names\",\n",
    "    y=\"girls_names\",\n",
    "    marginal_x=\"histogram\",\n",
    "    marginal_y=\"histogram\",\n",
    "    # color = color,\n",
    "    labels={\n",
    "        \"boys_names\": \"Enrichment Score (Boys Names)\",\n",
    "        \"girls_names\": \"Enrichment Score (Girls Names)\",\n",
    "    },\n",
    "    height=600,\n",
    "    width=800,\n",
    "    hover_name=\"feature\",\n",
    ")\n",
    "# reduce point size on the scatter only\n",
    "fig.update_traces(marker=dict(size=3), selector=dict(mode=\"markers\"))\n",
    "# annotate any features where the absolute distance between boys names and girls names > 3\n",
    "for feature in df_enrichment_scores.columns:\n",
    "    if abs(tmp_df[\"boys_names\"][feature] - tmp_df[\"girls_names\"][feature]) > 2.9:\n",
    "        fig.add_annotation(\n",
    "            x=tmp_df[\"boys_names\"][feature] - 0.4,\n",
    "            y=tmp_df[\"girls_names\"][feature] + 0.1,\n",
    "            text=f\"{feature}\",\n",
    "            showarrow=False,\n",
    "        )\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwVcsxnkBSdG"
   },
   "source": [
    "## Digging into Particular Features\n",
    "\n",
    "When we do these enrichments, I generate the logit weight histograms by category using the following function. It's important to make sure the categories you group by are in the columns of df_enrichment_scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgndTFdFBSdG"
   },
   "outputs": [],
   "source": [
    "for category in [\"boys_names\"]:\n",
    "    plot_top_k_feature_projections_by_token_and_category(\n",
    "        token_set_selected,\n",
    "        df_enrichment_scores,\n",
    "        category=category,\n",
    "        dec_projection_onto_W_U=dec_projection_onto_W_U,\n",
    "        model=model,\n",
    "        log_y=False,\n",
    "        histnorm=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKP3u0D2BSdG"
   },
   "source": [
    "# Appendix Results: Logit Weight distribution Statistics Accross All Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vhht2iCQBSdG"
   },
   "outputs": [],
   "source": [
    "W_U_stats_df_dec_all_layers = get_all_stats_dfs(\n",
    "    gpt2_small_sparse_autoencoders, gpt2_small_sae_sparsities, model, cosine_sim=True\n",
    ")\n",
    "\n",
    "display(W_U_stats_df_dec_all_layers.shape)\n",
    "display(W_U_stats_df_dec_all_layers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ok3DgORLBSdG"
   },
   "outputs": [],
   "source": [
    "# Let's plot the percentiles of the skewness and kurtosis by layer\n",
    "tmp_df = W_U_stats_df_dec_all_layers.groupby(\"layer\")[\"skewness\"].describe(\n",
    "    percentiles=[0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    ")\n",
    "tmp_df = tmp_df[[\"1%\", \"5%\", \"10%\", \"25%\", \"50%\", \"75%\", \"90%\", \"95%\", \"99%\"]]\n",
    "\n",
    "fig = px.area(\n",
    "    tmp_df,\n",
    "    title=\"Skewness by Layer\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    color_discrete_sequence=px.colors.sequential.Turbo,\n",
    ").show()\n",
    "\n",
    "\n",
    "tmp_df = W_U_stats_df_dec_all_layers.groupby(\"layer\")[\"kurtosis\"].describe(\n",
    "    percentiles=[0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    ")\n",
    "tmp_df = tmp_df[[\"1%\", \"5%\", \"10%\", \"25%\", \"50%\", \"75%\", \"90%\", \"95%\", \"99%\"]]\n",
    "\n",
    "fig = px.area(\n",
    "    tmp_df,\n",
    "    title=\"Kurtosis by Layer\",\n",
    "    width=800,\n",
    "    height=600,\n",
    "    color_discrete_sequence=px.colors.sequential.Turbo,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOkRZiunBSdG"
   },
   "outputs": [],
   "source": [
    "# let's make a pretty color scheme\n",
    "from plotly.colors import n_colors\n",
    "\n",
    "colors = n_colors(\"rgb(5, 200, 200)\", \"rgb(200, 10, 10)\", 13, colortype=\"rgb\")\n",
    "\n",
    "# Make a box plot of the skewness by layer\n",
    "fig = px.box(\n",
    "    W_U_stats_df_dec_all_layers,\n",
    "    x=\"layer\",\n",
    "    y=\"skewness\",\n",
    "    color=\"layer\",\n",
    "    color_discrete_sequence=colors,\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    title=\"Skewness cos(W_U,W_dec) by Layer in GPT2 Small Residual Stream SAEs\",\n",
    "    labels={\"layer\": \"Layer\", \"skewnss\": \"Skewness\"},\n",
    ")\n",
    "fig.update_xaxes(showticklabels=True, dtick=1)\n",
    "\n",
    "# increase font size\n",
    "fig.update_layout(font=dict(size=16))\n",
    "fig.show()\n",
    "\n",
    "# Make a box plot of the skewness by layer\n",
    "fig = px.box(\n",
    "    W_U_stats_df_dec_all_layers,\n",
    "    x=\"layer\",\n",
    "    y=\"kurtosis\",\n",
    "    color=\"layer\",\n",
    "    color_discrete_sequence=colors,\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    log_y=True,\n",
    "    title=\"log kurtosis cos(W_U,W_dec) by Layer in GPT2 Small Residual Stream SAEs\",\n",
    "    labels={\"layer\": \"Layer\", \"kurtosis\": \"Log Kurtosis\"},\n",
    ")\n",
    "fig.update_xaxes(showticklabels=True, dtick=1)\n",
    "\n",
    "# increase font size\n",
    "fig.update_layout(font=dict(size=16))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYNYdY3wBSdG"
   },
   "outputs": [],
   "source": [
    "# scatter\n",
    "fig = px.scatter(\n",
    "    W_U_stats_df_dec_all_layers[W_U_stats_df_dec_all_layers.log_feature_sparsity >= -9],\n",
    "    # W_U_stats_df_dec_all_layers[W_U_stats_df_dec_all_layers.layer == 8],\n",
    "    x=\"skewness\",\n",
    "    y=\"kurtosis\",\n",
    "    color=\"std\",\n",
    "    color_continuous_scale=\"Portland\",\n",
    "    hover_name=\"feature\",\n",
    "    # color_continuous_midpoint = 0,\n",
    "    # range_color = [-4,-1],\n",
    "    log_y=True,\n",
    "    height=800,\n",
    "    # width = 2000,\n",
    "    # facet_col=\"layer\",\n",
    "    # facet_col_wrap=5,\n",
    "    animation_frame=\"layer\",\n",
    ")\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.for_each_yaxis(lambda yaxis: yaxis.update(showticklabels=True))\n",
    "\n",
    "# decrease point size\n",
    "fig.update_traces(marker=dict(size=5))\n",
    "fig.show()\n",
    "fig.write_html(\"skewness_kurtosis_scatter_all_layers.html\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
